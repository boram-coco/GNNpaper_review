[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GNNpaper_review",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n \n\n\n(연구&보람) 결과시각화 – Experiment 1\n\n\n \n\n\n\n\nApr 11, 2024\n\n\ngraft\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-04-11-결과시각화-- 실험1.out.html",
    "href": "posts/2024-04-11-결과시각화-- 실험1.out.html",
    "title": "(연구&보람) 결과시각화 – Experiment 1",
    "section": "",
    "text": "(연구&보람) 결과시각화 – Experiment 1\n김보람\n2024-04-03\n\nimport pandas as pd\nimport numpy as np\n#import sklearn\nimport pickle \nimport time \nimport datetime\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\n\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\npd.options.plotting.backend = \"plotly\"\npio.templates.default = \"plotly_white\"\n\n\ndf = pd.read_csv('./240423_meged.csv')\ndf = df[['L2' not in l and 'L1' not in l for l in df.model]]\n#df['diff'] = abs(merged_df['train_frate'] - merged_df['test_frate'])\ndf = df[(df.test_frate &lt;= 0.005) & (0.009 &lt; df.train_frate) & (df.train_frate &lt; 0.51)]\n\n\ndf\n\n\n\n\n\n\n\n\nmodel\ntime\nacc\npre\nrec\nf1\nauc\ngraph_based\nmethod\nthrow_rate\ntrain_size\ntrain_cols\ntrain_frate\ntest_size\ntest_frate\nhyper_params\ntheta\ngamma\n\n\n\n\n14\nKNeighborsUnif\nNaN\n0.982984\n0.187500\n0.726562\n0.298077\n0.904913\nFalse\nAutogluon\n0.070000\n60060\n['amt']\n0.097869\n25740\n0.004973\nNaN\nNaN\nNaN\n\n\n15\nKNeighborsDist\nNaN\n0.980109\n0.160777\n0.710938\n0.262248\n0.885596\nFalse\nAutogluon\n0.070000\n60060\n['amt']\n0.097869\n25740\n0.004973\nNaN\nNaN\nNaN\n\n\n16\nLightGBMXT\nNaN\n0.980653\n0.154851\n0.648438\n0.250000\n0.949992\nFalse\nAutogluon\n0.070000\n60060\n['amt']\n0.097869\n25740\n0.004973\nNaN\nNaN\nNaN\n\n\n17\nLightGBM\nNaN\n0.985237\n0.204225\n0.679688\n0.314079\n0.963999\nFalse\nAutogluon\n0.070000\n60060\n['amt']\n0.097869\n25740\n0.004973\nNaN\nNaN\nNaN\n\n\n18\nRandomForestGini\nNaN\n0.979681\n0.147950\n0.648438\n0.240929\n0.929461\nFalse\nAutogluon\n0.070000\n60060\n['amt']\n0.097869\n25740\n0.004973\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44700\nExtraTreesEntr\nNaN\n0.994760\n0.122807\n0.241379\n0.162791\n0.763820\nFalse\nAutogluon\n0.008901\n500499\n['amt']\n0.009087\n13741\n0.002110\nNaN\nNaN\nNaN\n\n\n44701\nNeuralNetFastAI\nNaN\n0.997890\n0.000000\n0.000000\n0.000000\n0.917705\nFalse\nAutogluon\n0.008901\n500499\n['amt']\n0.009087\n13741\n0.002110\nNaN\nNaN\nNaN\n\n\n44702\nXGBoost\nNaN\n0.996580\n0.178571\n0.172414\n0.175439\n0.939455\nFalse\nAutogluon\n0.008901\n500499\n['amt']\n0.009087\n13741\n0.002110\nNaN\nNaN\nNaN\n\n\n44703\nNeuralNetTorch\nNaN\n0.997162\n0.187500\n0.103448\n0.133333\n0.941711\nFalse\nAutogluon\n0.008901\n500499\n['amt']\n0.009087\n13741\n0.002110\nNaN\nNaN\nNaN\n\n\n44704\nLightGBMLarge\nNaN\n0.997890\n0.000000\n0.000000\n0.000000\n0.944964\nFalse\nAutogluon\n0.008901\n500499\n['amt']\n0.009087\n13741\n0.002110\nNaN\nNaN\nNaN\n\n\n\n\n21062 rows × 18 columns\n\n\n\n\n카테고리별로 정리, 모델별!\n\n\nfig = px.scatter(\n    df,\n    x='train_frate',y='auc',\n    color='method',\n#    hover_data='f1',\n    opacity=0.2,\n    #---#\n    width = 750,\n    height = 800  \n)\n#fig.data[3]['marker']['opacity'] = 0.7\n#fig.data[3]['marker']['size'] = 9\nfig\n\n                                                \n\n\n\ndf2 = pd.read_csv('./240423_meged.csv')\ndf2 = df[['L2' not in l and 'L1' not in l for l in df.model]]\n#df['diff'] = abs(merged_df['train_frate'] - merged_df['test_frate'])\ndf3 = df2[(df2.test_frate &lt;= 0.005) & (df2.train_frate &lt;= 0.1)]\n\n\ndf3 = df3.drop_duplicates()\n\n\nfig2 = px.scatter(\n    df3,\n    x='train_frate',y='f1',\n    color='method',\n#    hover_data='model',\n    opacity=0.2,\n    #---#\n    width = 750,\n    height = 800  \n)\nfig2"
  },
  {
    "objectID": "posts/231111.html",
    "href": "posts/231111.html",
    "title": "graft",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport pickle\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\nimport graft\n\n\n%run ~/Dropbox/GNNpaper/posts/function_proposed_gcn.py\n\n\nwith open('./fraudTrain.pkl', 'rb') as file:\n    fraudTrain = pickle.load(file)    \n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\ntrain_mask, test_mask = mask(df50)"
  },
  {
    "objectID": "posts/231111.html#데이터정리",
    "href": "posts/231111.html#데이터정리",
    "title": "graft",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\ntrain_mask, test_mask = mask(df50)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]